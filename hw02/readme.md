# 【NLP作业-02】利用EM算法进行参数估计

<center>杨思捷&nbsp&nbsp&nbspZY2103533<center/>

## 引言

　极大似然估计是利用已知的样本结果，去反推最有可能（最大概率）导致这样结果的参数值，也就是在给定的观测变量下去估计参数值。然而现实中可能存在这样的问题，除了观测变量之外，还存在着未知的隐变量，因为变量未知，因此无法直接通过最大似然估计直接求参数值。EM算法是一种迭代算法，用于含有隐变量的概率模型的极大似然估计，或者说是极大后验概率估计。

## EM算法

 最大期望算法（Expectation-maximization algorithm，又译为期望最大化算法），是在概率模型中寻找参数最大似然估计或者最大后验估计的算法，其中概率模型依赖于无法观测的隐性变量。

最大期望算法经过两个步骤交替进行计算：

第一步是计算期望（E），利用对隐藏变量的现有估计值，计算其最大似然估计值；

第二步是最大化（M），最大化在E步上求得的最大似然值来计算参数的值。M步上找到的参数估计值被用于下一个E步计算中，这个过程不断交替进行。

### 公式推导

公式推导参加下面的手写

![image-20220419212625819](https://s2.loli.net/2022/04/19/xGjCAR9WNrZ3Utg.png)

## 模型分析

一个袋子中三种硬币的混合比例为：s1, s2与1-s1-s2 (0<=si<=1), 三种硬币掷出正面的概率分别为：p, q, r。 自己指定系数s1, s2, p, q, r，生成N个投掷硬币的结果（由01构成的序列，其中1为正面，0为反面），利用EM算法来对参数进行估计并与预先假定的参数进行比较。

该模型可以视为3个二项分布的混合，二项分布的概率密度函数为
$$
P(X=1)=p;P(X=0)=1-p;
$$

在该模型中设$\alpha_k$是第k个模型被选中的概率，$\alpha=[s_1,s_2,1-s_1-s_2]$，参数$\theta_k$是第k个模型对应的参数，$\theta = [p,q,r]$。设观测数据为$y_j（j = 1，2，...，N）$，则有隐变量$\gamma_{jk}$。$\gamma_{jk}$表示第j个观测来自第k个模型的概率。

因而EM算法的步骤如下，首先给定一个初始的$\theta$，安装当前的模型参数计算隐变量的期望
$$
\hat \gamma_{jk} = \frac{\alpha_k \phi(y_i|\theta_k)}{\sum_k \alpha_k \phi(y_i|\theta_k)}
$$
其中，$\phi(y_i|\theta_k)$为概率密度函数。然后根据隐变量的期望更新对应的参数
$$
\theta_k = \frac{\sum_k \hat \gamma_{jk} y_j}{\sum_k \hat \gamma_{jk}}
\\
\alpha_k = \frac{\sum_k \hat \gamma_{jk}}{N}
$$
重复上面的步骤，直到参数值收敛。

## 结果分析

该部分的源代码参见`MultiBinomial.py`运行完成后会生成一个中间记录结果的文件，作图脚本为`plot_iter.py`。

在给定真值$\alpha=[0.5, 0.3, 0.2]$和$\theta=[0.7, 0.9, 0.4]$的情况下，生成相关数据，总的取1000枚硬币，每枚硬币抛50次。迭代求解的结果如下图

当迭代条件为$\alpha=[0.3, 0.3, 0.4]$和$\theta=[0.8, 0.8, 0.5]$时的迭代结果如下

![2022-04-19_22-46-45](https://s2.loli.net/2022/04/19/68YuDRiAo1qraNU.png)

其中横坐标为迭代次数，纵坐标为参数值，虚线为真值。可以看到，由于初始条件相同，后续迭代值会一样，无法区分。而且距离真值也还有一定的距离。

当迭代条件为$\alpha=[0.3, 0.3, 0.4]$和$\theta=[0.8, 0.5, 0.5]$时的迭代结果如下

![2022-04-19_22-48-16](https://s2.loli.net/2022/04/19/hwZIOsq5GgRPYtX.png)

虽然初始值$\alpha$中有相同值，但在后续的迭代中可以分开，并且$\theta$也区分开了。相比于上一组迭代值，这组迭代值也更接近于真值。

上述两个结果和资料中的EM算法初值敏感，迭代可能陷入了局部最优点相对应。

## 参考资料

https://blog.csdn.net/zhihua_oba/article/details/73776553

https://blog.csdn.net/u010834867/article/details/90762296

https://www.cnblogs.com/jiangxinyang/p/9278608.html
